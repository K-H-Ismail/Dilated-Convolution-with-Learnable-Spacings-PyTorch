{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from termcolor import colored\n",
    "import torch\n",
    "import torch.autograd.profiler as profiler\n",
    "\n",
    "from modules.Swc2d import Swc2d\n",
    "from modules.Dcls2dFull import Dcls2dFull\n",
    "\n",
    "\n",
    "\n",
    "assert torch.cuda.is_available()\n",
    "cuda_device = torch.device(\"cuda\")  # device object representing GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_channels = 1\n",
    "out_channels = 1\n",
    "kernel_size = (2,2)\n",
    "dilation = (2,2)\n",
    "stride = (1,1)\n",
    "padding = (0,0)\n",
    "groups = 1\n",
    "bias = False\n",
    "\n",
    "m = torch.nn.Conv2d(in_channels=in_channels,\n",
    "              out_channels=out_channels,\n",
    "              kernel_size=kernel_size,\n",
    "              dilation=dilation,\n",
    "              stride=stride,\n",
    "              padding=padding,\n",
    "              groups=groups,\n",
    "              bias=bias).to(cuda_device)\n",
    "\n",
    "n = Swc2d(in_channels=in_channels,\n",
    "              out_channels=out_channels,\n",
    "              kernel_size=kernel_size,\n",
    "              dilation=dilation,\n",
    "              stride=stride,\n",
    "              padding=padding,\n",
    "              groups=groups,\n",
    "              bias=bias).to(cuda_device)\n",
    "\n",
    "\n",
    "\n",
    "X1 = torch.nn.Parameter(\n",
    "                      torch.tensor([[[[1., 2., 3., 4.],\n",
    "                                    [5., 6., 7., 8.], \n",
    "                                    [9., 10., 11., 12.],\n",
    "                                    [13., 14., 15., 16.]]]],device=cuda_device),\n",
    "                      requires_grad = True) \n",
    "X2 = torch.nn.Parameter(\n",
    "                      torch.tensor([[[[1., 2., 3., 4.],\n",
    "                                    [5., 6., 7., 8.], \n",
    "                                    [9., 10., 11., 12.],\n",
    "                                    [13., 14., 15., 16.]]]],device=cuda_device),\n",
    "                      requires_grad = True) \n",
    "\n",
    "m.weight = torch.nn.Parameter(\n",
    "                      torch.tensor([[[[20., 40.],\n",
    "                                    [60., 80.]]]],device=cuda_device),\n",
    "                      requires_grad = True)\n",
    "n.weight = torch.nn.Parameter(\n",
    "                      torch.tensor([[[[20., 40.],\n",
    "                                    [60., 80.]]]],device=cuda_device),\n",
    "                      requires_grad = True)\n",
    "\n",
    "back_truth = torch.nn.Parameter(\n",
    "                      torch.tensor([[[[1., 2.],\n",
    "                                    [4., 5.]]]],device=cuda_device),\n",
    "                      requires_grad = True)\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=True, profile_memory=True) as prof:\n",
    "    var2 = (n(X2) - back_truth).norm()\n",
    "var1 = (m(X1) - back_truth).norm()\n",
    "\n",
    "var1.backward();\n",
    "var2.backward();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1, 4, 4])\n",
      "torch.Size([1, 1, 2, 2])\n",
      "torch.Size([1, 1, 2, 2])\n",
      "torch.Size([1, 1, 2, 2])\n",
      "tensor([[[[1560., 1760.],\n",
      "          [2360., 2560.]]]], device='cuda:0',\n",
      "       grad_fn=<CudnnConvolutionBackward>)\n",
      "torch.Size([1, 1, 2, 2])\n",
      "tensor([[[[1560., 1760.],\n",
      "          [2360., 2560.]]]], device='cuda:0', grad_fn=<swc2dBackward>)\n",
      "tensor([[[[ 7.6718, 11.5944],\n",
      "          [23.3621, 27.2847]]]], device='cuda:0')\n",
      "tensor([[[[0., 0.],\n",
      "          [0., 0.]]]], device='cuda:0')\n",
      "tensor([[[[ 7.4323,  8.3810, 14.8646, 16.7620],\n",
      "          [11.2319, 12.1806, 22.4637, 24.3611],\n",
      "          [22.2968, 25.1429, 29.7291, 33.5239],\n",
      "          [33.6956, 36.5417, 44.9274, 48.7222]]]], device='cuda:0')\n",
      "tensor([[[[0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.],\n",
      "          [0., 0., 0., 0.]]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(X1.size())\n",
    "print(m.weight.size())\n",
    "print(n.weight.size())\n",
    "\n",
    "print(m(X1).size())\n",
    "print(m(X1))\n",
    "print(n(X2).size())\n",
    "print(n(X2))\n",
    "\n",
    "\n",
    "print(m.weight.grad) \n",
    "print(n.weight.grad)\n",
    "\n",
    "print(X1.grad) \n",
    "print(X2.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n.weight.nonzero().size(0)*100/n.weight.numel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = 16\n",
    "in_channels = 2**9\n",
    "out_channels = 2**10\n",
    "kernel_size = (3,3)\n",
    "dilation = (8,8)\n",
    "stride = (1,1)\n",
    "padding = (0,0)\n",
    "groups = 1\n",
    "bias = False\n",
    "h = 200\n",
    "w = 200\n",
    "h_o = int((h + 2 * padding[0] - (dilation[0] * (kernel_size[0] - 1) + 1)) / stride[0] + 1)\n",
    "w_o = int((w + 2 * padding[1] - (dilation[1] * (kernel_size[1] - 1) + 1)) / stride[1] + 1)\n",
    "\n",
    "n = Swc2d(in_channels=in_channels,\n",
    "              out_channels=out_channels,\n",
    "              kernel_size=kernel_size,\n",
    "              dilation=dilation,\n",
    "              stride=stride,\n",
    "              padding=padding,\n",
    "              groups=groups,\n",
    "              bias=bias).to(cuda_device)\n",
    "\n",
    "X2 = torch.nn.Parameter(torch.rand(batch,in_channels,h,w,device=cuda_device), requires_grad = True)\n",
    "back_truth = torch.nn.Parameter(torch.rand(batch,out_channels,h_o,w_o,device=cuda_device), requires_grad = True)\n",
    "\n",
    "with torch.autograd.profiler.profile(use_cuda=True, profile_memory=True) as prof:\n",
    "    var2 = (n(X2) - back_truth).norm()\n",
    "    var2.backward();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |    6768 MB |    9732 MB |   45696 MB |   38928 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |    6768 MB |    9732 MB |   45696 MB |   38928 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |    9734 MB |    9734 MB |    9734 MB |       0 B  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |     849 MB |     867 MB |    2636 MB |    1786 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |      16    |      25    |     528    |     512    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |      16    |      25    |     528    |     512    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |       7    |       7    |       7    |       0    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |       6    |       7    |     277    |     271    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.memory_summary(device=cuda_device, abbreviated=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                               Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "                              swc2d         1.81%      35.057us         2.47%      47.811us      47.811us      51.328us         0.12%      51.328us      51.328us           0 b           0 b       2.07 Gb           0 b             1  \n",
      "                         aten::view         2.61%      50.557us         2.61%      50.557us      10.111us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
      "                        aten::empty         5.65%     109.522us         5.65%     109.522us      10.952us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       8.27 Gb       8.27 Gb            10  \n",
      "                          aten::sub         1.60%      31.078us         1.85%      35.862us      35.862us      11.582ms        28.00%      11.582ms      11.582ms           0 b           0 b       2.07 Gb           0 b             1  \n",
      "               aten::frobenius_norm         0.67%      13.056us         5.76%     111.609us     111.609us       5.504us         0.01%       3.684ms       3.684ms           0 b           0 b       1.00 Kb           0 b             1  \n",
      "                         aten::norm         3.24%      62.703us         3.54%      68.657us      68.657us       3.672ms         8.88%       3.672ms       3.672ms           0 b           0 b         512 b           0 b             1  \n",
      "                   aten::as_strided         0.08%       1.471us         0.08%       1.471us       1.471us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
      "                      aten::resize_         2.46%      47.604us         2.46%      47.604us      15.868us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       2.07 Gb       2.07 Gb             3  \n",
      "                        aten::copy_         1.15%      22.211us         1.15%      22.211us      22.211us       6.112us         0.01%       6.112us       6.112us           0 b           0 b           0 b           0 b             1  \n",
      "                    aten::ones_like         0.41%       7.996us         1.31%      25.292us      25.292us       3.456us         0.01%       7.552us       7.552us           0 b           0 b         512 b           0 b             1  \n",
      "                   aten::empty_like         1.28%      24.808us         4.54%      87.984us      21.996us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       1.24 Gb           0 b             4  \n",
      "                aten::empty_strided         3.26%      63.176us         3.26%      63.176us      15.794us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       1.24 Gb       1.24 Gb             4  \n",
      "                        aten::fill_         6.85%     132.681us         6.85%     132.681us      26.536us       2.249ms         5.44%       2.249ms     449.728us           0 b           0 b           0 b           0 b             5  \n",
      "         torch::autograd::GraphRoot         0.16%       3.055us         0.16%       3.055us       3.055us       2.048us         0.00%       2.048us       2.048us           0 b           0 b           0 b           0 b             1  \n",
      "     torch::autograd::CopyBackwards         2.56%      49.679us         3.32%      64.323us      64.323us       2.111us         0.01%       4.160us       4.160us           0 b           0 b           0 b           0 b             1  \n",
      "                           aten::to         0.76%      14.644us         0.76%      14.644us      14.644us       2.049us         0.00%       2.049us       2.049us           0 b           0 b           0 b           0 b             1  \n",
      "                      NormBackward1         4.69%      90.810us        26.74%     517.888us     517.888us      10.143us         0.02%       7.956ms       7.956ms           0 b           0 b       2.07 Gb      -1.00 Kb             1  \n",
      "                          aten::div         4.40%      85.285us         5.73%     111.043us     111.043us       8.064us         0.02%       8.064us       8.064us           0 b           0 b         512 b           0 b             1  \n",
      "                           aten::eq         6.62%     128.152us        11.40%     220.838us     110.419us      12.960us         0.03%      17.536us       8.768us           0 b           0 b       1.00 Kb           0 b             2  \n",
      "                 aten::masked_fill_         3.03%      58.597us         3.03%      58.597us      58.597us       6.305us         0.02%       6.305us       6.305us           0 b           0 b           0 b           0 b             1  \n",
      "                          aten::mul         9.68%     187.394us        11.50%     222.707us     111.354us      15.806ms        38.20%      15.806ms       7.903ms           0 b           0 b       4.13 Gb           0 b             2  \n",
      "                       SubBackward0         2.12%      41.081us        18.10%     350.638us     350.638us       6.051us         0.01%      15.776ms      15.776ms           0 b           0 b       2.07 Gb      -2.07 Gb             1  \n",
      "                          aten::neg         8.47%     164.000us        13.74%     266.156us     133.078us       7.883ms        19.05%      15.760ms       7.880ms           0 b           0 b       4.13 Gb           0 b             2  \n",
      "    torch::autograd::AccumulateGrad         4.47%      86.639us        10.05%     194.741us      48.685us      13.738us         0.03%      30.340us       7.585us           0 b           0 b           0 b           0 b             4  \n",
      "                       aten::detach         3.49%      67.622us         5.58%     108.102us      27.026us      13.176us         0.03%      16.602us       4.150us           0 b           0 b           0 b           0 b             4  \n",
      "                             detach         2.09%      40.480us         2.09%      40.480us      10.120us       3.426us         0.01%       3.426us       0.856us           0 b           0 b           0 b           0 b             4  \n",
      "                      swc2dBackward         8.58%     166.152us        30.24%     585.618us     585.618us       7.844us         0.02%       2.277ms       2.277ms           0 b           0 b       1.24 Gb    -132.50 Kb             1  \n",
      "                   aten::zeros_like         3.46%      66.969us        14.73%     285.204us      95.068us      13.887us         0.03%       2.259ms     753.108us           0 b           0 b       1.24 Gb           0 b             3  \n",
      "                        aten::zero_         3.01%      58.339us         7.15%     138.447us      46.149us       7.039us         0.02%       2.245ms     748.479us           0 b           0 b           0 b           0 b             3  \n",
      "                         aten::ones         1.34%      26.019us         4.63%      89.663us      89.663us       4.098us         0.01%      10.242us      10.242us           0 b           0 b     132.50 Kb           0 b             1  \n",
      "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
      "Self CPU time total: 1.937ms\n",
      "CUDA time total: 41.371ms\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(prof.key_averages().table( row_limit=1000))\n",
    "#prof.export_chrome_trace(\"trace.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                               Name    Self CPU %      Self CPU   CPU total %     CPU total  CPU time avg     Self CUDA   Self CUDA %    CUDA total  CUDA time avg       CPU Mem  Self CPU Mem      CUDA Mem  Self CUDA Mem    # of Calls  \n",
    "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "                              swc2d         1.81%      35.057us         2.47%      47.811us      47.811us      51.328us         0.12%      51.328us      51.328us           0 b           0 b       2.07 Gb           0 b             1  \n",
    "                         aten::view         2.61%      50.557us         2.61%      50.557us      10.111us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             5  \n",
    "                        aten::empty         5.65%     109.522us         5.65%     109.522us      10.952us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       8.27 Gb       8.27 Gb            10  \n",
    "                          aten::sub         1.60%      31.078us         1.85%      35.862us      35.862us      11.582ms        28.00%      11.582ms      11.582ms           0 b           0 b       2.07 Gb           0 b             1  \n",
    "               aten::frobenius_norm         0.67%      13.056us         5.76%     111.609us     111.609us       5.504us         0.01%       3.684ms       3.684ms           0 b           0 b       1.00 Kb           0 b             1  \n",
    "                         aten::norm         3.24%      62.703us         3.54%      68.657us      68.657us       3.672ms         8.88%       3.672ms       3.672ms           0 b           0 b         512 b           0 b             1  \n",
    "                   aten::as_strided         0.08%       1.471us         0.08%       1.471us       1.471us       0.000us         0.00%       0.000us       0.000us           0 b           0 b           0 b           0 b             1  \n",
    "                      aten::resize_         2.46%      47.604us         2.46%      47.604us      15.868us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       2.07 Gb       2.07 Gb             3  \n",
    "                        aten::copy_         1.15%      22.211us         1.15%      22.211us      22.211us       6.112us         0.01%       6.112us       6.112us           0 b           0 b           0 b           0 b             1  \n",
    "                    aten::ones_like         0.41%       7.996us         1.31%      25.292us      25.292us       3.456us         0.01%       7.552us       7.552us           0 b           0 b         512 b           0 b             1  \n",
    "                   aten::empty_like         1.28%      24.808us         4.54%      87.984us      21.996us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       1.24 Gb           0 b             4  \n",
    "                aten::empty_strided         3.26%      63.176us         3.26%      63.176us      15.794us       0.000us         0.00%       0.000us       0.000us           0 b           0 b       1.24 Gb       1.24 Gb             4  \n",
    "                        aten::fill_         6.85%     132.681us         6.85%     132.681us      26.536us       2.249ms         5.44%       2.249ms     449.728us           0 b           0 b           0 b           0 b             5  \n",
    "         torch::autograd::GraphRoot         0.16%       3.055us         0.16%       3.055us       3.055us       2.048us         0.00%       2.048us       2.048us           0 b           0 b           0 b           0 b             1  \n",
    "     torch::autograd::CopyBackwards         2.56%      49.679us         3.32%      64.323us      64.323us       2.111us         0.01%       4.160us       4.160us           0 b           0 b           0 b           0 b             1  \n",
    "                           aten::to         0.76%      14.644us         0.76%      14.644us      14.644us       2.049us         0.00%       2.049us       2.049us           0 b           0 b           0 b           0 b             1  \n",
    "                      NormBackward1         4.69%      90.810us        26.74%     517.888us     517.888us      10.143us         0.02%       7.956ms       7.956ms           0 b           0 b       2.07 Gb      -1.00 Kb             1  \n",
    "                          aten::div         4.40%      85.285us         5.73%     111.043us     111.043us       8.064us         0.02%       8.064us       8.064us           0 b           0 b         512 b           0 b             1  \n",
    "                           aten::eq         6.62%     128.152us        11.40%     220.838us     110.419us      12.960us         0.03%      17.536us       8.768us           0 b           0 b       1.00 Kb           0 b             2  \n",
    "                 aten::masked_fill_         3.03%      58.597us         3.03%      58.597us      58.597us       6.305us         0.02%       6.305us       6.305us           0 b           0 b           0 b           0 b             1  \n",
    "                          aten::mul         9.68%     187.394us        11.50%     222.707us     111.354us      15.806ms        38.20%      15.806ms       7.903ms           0 b           0 b       4.13 Gb           0 b             2  \n",
    "                       SubBackward0         2.12%      41.081us        18.10%     350.638us     350.638us       6.051us         0.01%      15.776ms      15.776ms           0 b           0 b       2.07 Gb      -2.07 Gb             1  \n",
    "                          aten::neg         8.47%     164.000us        13.74%     266.156us     133.078us       7.883ms        19.05%      15.760ms       7.880ms           0 b           0 b       4.13 Gb           0 b             2  \n",
    "    torch::autograd::AccumulateGrad         4.47%      86.639us        10.05%     194.741us      48.685us      13.738us         0.03%      30.340us       7.585us           0 b           0 b           0 b           0 b             4  \n",
    "                       aten::detach         3.49%      67.622us         5.58%     108.102us      27.026us      13.176us         0.03%      16.602us       4.150us           0 b           0 b           0 b           0 b             4  \n",
    "                             detach         2.09%      40.480us         2.09%      40.480us      10.120us       3.426us         0.01%       3.426us       0.856us           0 b           0 b           0 b           0 b             4  \n",
    "                      swc2dBackward         8.58%     166.152us        30.24%     585.618us     585.618us       7.844us         0.02%       2.277ms       2.277ms           0 b           0 b       1.24 Gb    -132.50 Kb             1  \n",
    "                   aten::zeros_like         3.46%      66.969us        14.73%     285.204us      95.068us      13.887us         0.03%       2.259ms     753.108us           0 b           0 b       1.24 Gb           0 b             3  \n",
    "                        aten::zero_         3.01%      58.339us         7.15%     138.447us      46.149us       7.039us         0.02%       2.245ms     748.479us           0 b           0 b           0 b           0 b             3  \n",
    "                         aten::ones         1.34%      26.019us         4.63%      89.663us      89.663us       4.098us         0.01%      10.242us      10.242us           0 b           0 b     132.50 Kb           0 b             1  \n",
    "-----------------------------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  ------------  \n",
    "Self CPU time total: 1.937ms\n",
    "CUDA time total: 41.371ms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
